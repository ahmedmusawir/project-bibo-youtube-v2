# Session Log: 2026-02-01

## Objective
Stabilization PR for video generation pipeline - fix pytest collection, MoviePy compatibility, transcription bugs.

---

## 14:01 - Stabilization PR Started

### Changes Made

#### 1. Pytest Collection Fixes
- **`pyproject.toml`** - Changed `where = ["src"]` to `where = ["."]` with `include = ["src", "src.*"]` so `from src.xxx` imports work
- **`tests/integration/__init__.py`** - Created empty file to fix test module name collisions
- **`tests/unit/__init__.py`** - Created empty file to fix test module name collisions
- **`pyproject.toml`** - Added `addopts = "-m 'not integration'"` to exclude integration tests by default

#### 2. MoviePy 2.x Compatibility
- **`tests/integration/test_video_composition-1.py`** - Changed `from moviepy.editor import AudioFileClip` → `from moviepy import AudioFileClip`
- **`tests/unit/test_video_composition.py`** - Updated mock chain to use `with_audio` instead of `set_audio` (MoviePy 2.x API)

#### 3. Transcription Path Bug Fix (IsADirectoryError)
- **`main.py`** - Now passes `os.path.join(project_path, "0_transcript.txt")` instead of just `project_path`
- **`tests/unit/test_transcription.py`** - Added regression test `test_transcribe_rejects_directory_path`

### Result
- `pytest` runs 12 unit tests successfully, 8 integration tests deselected by default
- `pytest -m integration` selects only integration tests

---

## 21:10 - Transcription Large File Fix

### Problem
User reported `openai.BadRequestError: 400` when transcribing 1+ hour YouTube video. OpenAI Whisper API has 25MB file size limit.

### Changes Made

#### Audio Chunking Implementation
- **`src/transcription.py`**:
  - Added `pydub` import and `CHUNK_DURATION_MS = 10 * 60 * 1000` (10 minutes)
  - Added `_split_audio_into_chunks()` function
  - Modified `transcribe_youtube_audio()` to split, transcribe chunks, and combine results

- **`tests/unit/test_transcription.py`**:
  - Updated existing tests to mock `_split_audio_into_chunks`
  - Added `test_transcribe_combines_chunks` to verify chunk combining logic

### Result
- 13 unit tests pass
- Large audio files (1+ hour) will be split into 10-minute chunks for transcription

---

---

## 21:20 - yt-dlp Download Failure Fix

### Problem
YouTube download failed with `ERROR: The downloaded file is empty` due to:
- `nsig extraction failed`
- `SSAP (server-side ads) experiment` blocking downloads

This is a known YouTube anti-bot measure that requires updated yt-dlp.

### Fix
```bash
pip install --upgrade yt-dlp
```
Updated: `2025.6.30` → `2026.1.31`

---

## 22:44 - Summarization Provider Switch

### Problem
Anthropic API returned `400 - credit balance too low`.

### Fix
Switched `src/summarization.py` from Anthropic to Google Gemini:
- `ChatAnthropic` → `ChatGoogleGenerativeAI`
- Model: `gemini-3-flash-preview`
- Uses `GOOGLE_API_KEY` from `.env`

---

## 15:25 - Replace OpenAI TTS with Google Cloud TTS

### Changes Made

- **`src/text_to_speech.py`**: Complete rewrite
  - Replaced OpenAI TTS with Google Cloud Text-to-Speech
  - Uses ADC authentication (same as Vertex AI)
  - Added `list_voices()` and `print_voices()` helper functions
  - Voice configurable via env vars: `GOOGLE_TTS_VOICE`, `GOOGLE_TTS_LANG`
  - Default voice: `en-US-Studio-O` (high-quality Studio voice)
  - Chunk limit: 4500 chars (Google TTS limit is 5000 bytes)

- **`tests/unit/test_text_to_speech.py`**: Updated mocks
  - Now mocks `_get_tts_client()` instead of OpenAI client
  - Added `test_split_text_respects_limit` and `test_split_text_splits_long_text`

- **`requirements.txt`**: Added `google-cloud-texttospeech`

### Result
- 3 unit tests pass
- Pipeline contract unchanged: `synthesize_speech(summary_path, audio_path)`

---

## 16:32 - Replace OpenAI Whisper with Google Cloud Speech-to-Text

### Changes Made

- **`src/transcription.py`**: Complete rewrite
  - Replaced OpenAI Whisper with Google Cloud Speech-to-Text
  - Uses long-running recognition API (`long_running_recognize`)
  - Converts MP3 to FLAC (mono, 16kHz) for optimal STT performance
  - Model: `latest_long` (optimized for long-form video content)
  - Language configurable via `GOOGLE_STT_LANG` env var (default: `en-US`)
  - Automatic punctuation enabled
  - Uses ADC authentication (same as TTS and Vertex AI)
  - Removed audio chunking (Google STT handles long audio natively)

- **`tests/unit/test_transcription.py`**: Updated mocks
  - Now mocks `_get_speech_client()` and `_convert_to_flac()`
  - Updated all 3 tests to use Google Speech client mocks
  - Renamed `test_transcribe_combines_chunks` → `test_transcribe_multiple_results`

- **`requirements.txt`**: Added `google-cloud-speech`

### Why Long-Running Recognition?

Google Cloud Speech-to-Text has two modes:
1. **Synchronous** (`recognize`) - For audio < 60 seconds
2. **Long-running** (`long_running_recognize`) - For audio up to 480 minutes

We use long-running recognition because YouTube videos are typically > 1 minute. This API:
- Handles large files without chunking
- Returns results asynchronously (polls until complete)
- Supports `latest_long` model optimized for videos

### Result
- 3 unit tests pass
- Pipeline contract unchanged: `transcribe_youtube_audio(youtube_url, output_path)`
- No more 25MB Whisper limit or manual chunking

---

## 16:45 - Fix 10MB Inline Audio Limit with GCS Upload

### Problem
First test with real YouTube video failed:
```
google.api_core.exceptions.InvalidArgument: 400 Request payload size exceeds the limit: 10485760 bytes.
```

Google Cloud Speech-to-Text has a **10MB limit for inline audio**. For larger files, audio must be uploaded to Google Cloud Storage.

### Solution
Updated `src/transcription.py` to:
1. Check file size after FLAC conversion
2. If < 10MB: Use inline audio (existing behavior)
3. If > 10MB: Upload to GCS, use `uri` instead of `content`
4. Auto-cleanup GCS blob after transcription

### New Functions
- `_upload_to_gcs(file_path)` - Uploads to GCS bucket, returns URI and blob
- `_delete_from_gcs(blob)` - Cleans up temporary GCS files

### Configuration Required
Add to `.env`:
```bash
GOOGLE_STT_BUCKET=your-bucket-name
```

The bucket must:
- Exist in your GCP project
- Be accessible by your service account
- Have appropriate permissions (Storage Object Creator/Viewer)

### Result
- Unit tests still pass (mocks handle both inline and GCS paths)
- Large files now supported (up to 480 minutes)
- Automatic fallback: small files use inline, large files use GCS

---

## 23:54 - Fix Timeout for Long Videos

### Problem
62MB FLAC file (30-40 min video) timed out after 10 minutes:
```
TimeoutError: Operation did not complete within the designated timeout of 600 seconds.
```

### Solution
Increased `operation.result(timeout=...)` from 600s (10 min) to 3600s (60 min).

Google Cloud Speech-to-Text processing time scales with audio length:
- Short videos (5-10 min): ~2-5 minutes
- Medium videos (20-30 min): ~10-20 minutes  
- Long videos (60+ min): ~30-60 minutes

### Result
- Timeout now 60 minutes (handles videos up to ~2 hours)
- Added user-facing message about expected wait time

---

## Next Steps
- [ ] Retry large YouTube video transcription
- [ ] Full pipeline end-to-end test
